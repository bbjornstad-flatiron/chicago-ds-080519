{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math in the Trees!\n",
    "----\n",
    "In this notebook we combine math with esteemed bass producer Of the Trees. Except not really. I wish it were that interesting. Instead we are only covering the math of decision trees.\n",
    "\n",
    "In any case, we will start with ensemble methods in detail, and later we will discuss some more about the math and metrics used.\n",
    "\n",
    "Ensemble Methods Intro:\n",
    "- Ensemble methods use more than one classifier or regressor to build a model. \n",
    "- One of the most widely used in Kaggle competitions, as it increases accuracy highly.\n",
    "- These methods allow us to reduce both the variance and bias in the final models.\n",
    "An example:\n",
    "- We have a person who can consult one of three sources and wants to make an investment:\n",
    "  - Stock Broker -> 65% accuracy\n",
    "  - Company Expert -> 80% accuracy\n",
    "  - Industry Expert -> 85% accuracy\n",
    "  - In this case, the person should use the advice of the Industry Expert.\n",
    "- In another case, say that we can combine information from all 3:\n",
    "  - The overall error will be then be 0.35 * 0.20 * 0.15, which means that the overall accuracy is 1-0.0105, which is very close to 1.\n",
    "  \n",
    "Random Forests:\n",
    "- Decision trees have an issue of overfitting.\n",
    "- To rememdy this, we apply a bagging ensemble of decision trees\n",
    "- In each tree we take a resampling of the original data on both the samples and features axis (or at least both are possible)\n",
    "- If we want to take a classification, we take a majority vote from all the possible classifiers\n",
    "- If we want to take a regression, we take an average.\n",
    "- Decision trees are sensitive to variance, and random forests are the way that we can tackle this problem.\n",
    "\n",
    "Boosting: Pseudocode\n",
    "- Train a classifier\n",
    "- Check for misclassification errors\n",
    "- Assign higher weights to those data which were misclassified\n",
    "- Train a new classifier with the weights as assigned\n",
    "- Check for misclassification errors\n",
    "- Repeat until we have produced a sufficient model\n",
    "\n",
    "Boosting vs Bagging:\n",
    "- Boosting is an iterative process, where at each successive step a new classifier is trained using the weights produced by results from the previous classifier.\n",
    "- Boosting is a process where we are taking bootstrapped aggregation of the dataset, by taking subsets of samples and features and training a collection of classifiers on these subsets. This is not an iterative process in the same way that boosting is.\n",
    "\n",
    "Stacking:\n",
    "- Stacking is training a new classifier on the output of a previous classifier or series of classifiers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
