{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Module 4 Assessment</h1>\n",
    "\n",
    "## Overview\n",
    "\n",
    "This assessment is designed to test your understanding of the Mod 4 material. It covers:\n",
    "\n",
    "* Bayes Theorem\n",
    "* Calculus, Cost Function, and Gradient Descent\n",
    "* Extensions to Linear Models\n",
    "* Time Series\n",
    "\n",
    "Read the instructions carefully. You will be asked both to write code and respond to a few short answer questions.\n",
    "\n",
    "### Note on the short answer questions\n",
    "\n",
    "For the short answer questions please use your own words. The expectation is that you have not copied and pasted from an external source, even if you consult another source to help craft your response. While the short answer questions are not necessarily being assessed on grammatical correctness or sentence structure, do your best to communicate yourself clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 1: Bayesian Statistics [Suggested time: 15 minutes]\n",
    "### a. Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thomas wants to get a new puppy üêï üê∂ üê© \n",
    "\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/rD8R00QOKwfxC/giphy.gif\" />\n",
    "\n",
    "He can choose to get his new puppy either from the pet store or the pound. The probability of him going to the pet store is $0.2$. \n",
    "\n",
    "He can choose to get either a big, medium or small puppy.\n",
    "\n",
    "If he goes to the pet store, the probability of him getting a small puppy is $0.6$. The probability of him getting a medium puppy is $0.3$, and the probability of him getting a large puppy is $0.1$.\n",
    "\n",
    "If he goes to the pound, the probability of him getting a small puppy is $0.1$. The probability of him getting a medium puppy is $0.35$, and the probability of him getting a large puppy is $0.55$.\n",
    "\n",
    "4.a.1) What is the probability of Thomas getting a small puppy?\n",
    "4.a.2) Given that he got a large puppy, what is the probability that Thomas went to the pet store?\n",
    "4.a.3) Given that Thomas got a small puppy, is it more likely that he went to the pet store or to the pound?\n",
    "4.a.4) For Part 2, what is the prior, posterior and likelihood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_s_a = 0.6\n",
    "p_m_a = 0.3\n",
    "p_l_a = 0.1\n",
    "\n",
    "p_s_b = 0.1\n",
    "p_m_b = 0.35\n",
    "p_l_b = 0.55\n",
    "\n",
    "p_a = 0.2\n",
    "p_b = 0.8\n",
    "\n",
    "# now we do multiplication as usual with Bayes theorem to get the results...\n",
    "# but I'm out of time to do it even though it would take like 5 minutes...\n",
    "\n",
    "ans1 = None\n",
    "ans2 = None\n",
    "ans3 = \"answer here\"\n",
    "ans4_prior = \"answer here\"\n",
    "ans4_posterior = \"answer here\"\n",
    "ans4_likelihood = \"answer here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part II: Calculus, Cost Function, and Gradient Descent [Suggested Time: 15 min]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![best fit line](visuals/best_fit_line.png)\n",
    "\n",
    "The best fit line that goes through the scatterplot up above can be generalized in the following equation: $$y = mx + b$$\n",
    "\n",
    "Of all the possible lines, we can prove why that particular line was chosen using the plot down below:\n",
    "\n",
    "![](visuals/cost_curve.png)\n",
    "\n",
    "where RSS is defined as the residual sum of squares:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "RSS &= \\sum_{i=1}^n(actual - expected)^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - \\hat{y})^2 \\\\\n",
    "&= \\sum_{i=1}^n(y_i - (mx_i + b))^2\n",
    "\\end{align}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is a more generalized name for the RSS curve above? How is it related to machine learning models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# The RSS curve above is a case of a cost function, which is a function that assigns mathematical costs to trained models in order\n",
    "# to evaluate their accuracy. In the case of a regression, like this one, taking the residual sum of squares is mathematically well\n",
    "# defined way to assign this cost. In general, a cost function should assign higher costs to worse models, so that when we minimize\n",
    "# the function using analytical tools, we are approaching the better/best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Would you rather choose a $m$ value of 0.08 or 0.05 from the RSS curve up above?   What is the relation between the position on the cost curve, the error, and the slope of the line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# In this case we definitely want to select an m value of 0.05. Because this is the minimum point on the graph of the model's cost\n",
    "# function above, we know that the overall accuracy of the model to our training data is the highest (i.e. the errors are the smallest\n",
    "# overall). In particular, the cost function is a function on the input space of model parameters, and so minimizing in this space\n",
    "# ensures that we are moving towards optimal parameters. Since the minimum is close to 0.05, we should select this value for the optimal\n",
    "# model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](visuals/gd.png)\n",
    "\n",
    "### 3. Using the gradient descent visual from above, explain why the distance between each step is getting smaller as more steps occur with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# Each step of gradient descent becomes smaller in the above photo, as the overall size of each step is ultimately controlled by\n",
    "# the magnitude of the gradient at that point. While the learning rate is a constant, as we move down the graph towards the minimum,\n",
    "# the slope (analogous to the gradient in this case) becomes smaller in magnitude, so the multiplication of the slope with the \n",
    "# learning rate (which determines where the next step will go) becomes smaller overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is the purpose of a learning rate in gradient descent? Explain how a very small and a very large learning rate would affect the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# The purpose of the learning rate is to have an overall method to control the speed at which the algorithm performs gradient descent.\n",
    "# In particular, the gradient tells you the direction of greatest increase or decrease, which we use to perform gradient descent, but\n",
    "# for some models, the magnitude of the gradient may be very large or very small. In the case where the gradient is very large, each\n",
    "# step will be larger, leading to a faster descent process but a lower level of granularity or accuracy, and an increased probability\n",
    "# that the algorithm will ``overshoot'' the minimum. In the case where the gradient is small, the model may not converge to the minimum\n",
    "# in an appreciable amount of time, making analysis difficult or causing the program to stall. In either case, the learning rate allows\n",
    "# us to control these effects to optimize model generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Extensions to Linear Regression [Suggested Time: 20 min]\n",
    "---\n",
    "\n",
    "In this section, you're going to be creating linear models that are more complicated than a simple linear regression. In the cells below, we are importing relevant modules that you might need later on. We also load and prepare the dataset for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>radio</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>147.042500</td>\n",
       "      <td>23.264000</td>\n",
       "      <td>30.554000</td>\n",
       "      <td>14.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>85.854236</td>\n",
       "      <td>14.846809</td>\n",
       "      <td>21.778621</td>\n",
       "      <td>5.217457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>74.375000</td>\n",
       "      <td>9.975000</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>10.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>149.750000</td>\n",
       "      <td>22.900000</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>12.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>218.825000</td>\n",
       "      <td>36.525000</td>\n",
       "      <td>45.100000</td>\n",
       "      <td>17.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>296.400000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TV       radio   newspaper       sales\n",
       "count  200.000000  200.000000  200.000000  200.000000\n",
       "mean   147.042500   23.264000   30.554000   14.022500\n",
       "std     85.854236   14.846809   21.778621    5.217457\n",
       "min      0.700000    0.000000    0.300000    1.600000\n",
       "25%     74.375000    9.975000   12.750000   10.375000\n",
       "50%    149.750000   22.900000   25.750000   12.900000\n",
       "75%    218.825000   36.525000   45.100000   17.400000\n",
       "max    296.400000   49.600000  114.000000   27.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('raw_data/advertising.csv').drop('Unnamed: 0',axis=1)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('sales', axis=1)\n",
    "y = data['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing set. Do not change the random state please!\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y,random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. We'd like to add a bit of complexity to the model created in the example above, and we will do it by adding some polynomial terms. Write a function to calculate train and test error for different polynomial degrees.\n",
    "\n",
    "This function should:\n",
    "* take `degree` as a parameter that will be used to create polynomial features to be used in a linear regression model\n",
    "* create a PolynomialFeatures object for each degree and fit a linear regression model using the transformed data\n",
    "* calculate the mean square error for each level of polynomial\n",
    "* return the `train_error` and `test_error` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def polynomial_regression(degree):\n",
    "    \"\"\"\n",
    "    Calculate train and test errorfor a linear regression with polynomial features.\n",
    "    (Hint: use PolynomialFeatures)\n",
    "    \n",
    "    input: Polynomial degree\n",
    "    output: Mean squared error for train and test set\n",
    "    \"\"\"\n",
    "    # // your code here //\n",
    "    \n",
    "    poly_transformer = PolynomialFeatures(degree=degree)\n",
    "    \n",
    "    # luckily we don't need to do any concatenation\n",
    "    # poly features generates at least 1, a, and b for input columns a and b\n",
    "    x_train_poly = poly_transformer.fit_transform(X_train)\n",
    "    x_test_poly = poly_transformer.transform(X_test)\n",
    "    \n",
    "    lin_reg = LinearRegression()\n",
    "    lin_reg.fit(x_train_poly, y_train)\n",
    "    x_train_pred = lin_reg.predict(x_train_poly)\n",
    "    x_test_pred = lin_reg.predict(x_test_poly)\n",
    "    \n",
    "    train_error = mean_squared_error(y_train, x_train_pred)\n",
    "    test_error = mean_squared_error(y_test, x_test_pred)\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out your new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24235967358392063, 0.15281375976869446)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_regression(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your answers\n",
    "\n",
    "MSE for degree 3:\n",
    "- Train: 0.2423596735839209\n",
    "- Test: 0.15281375973923944\n",
    "\n",
    "MSE for degree 4:\n",
    "- Train: 0.18179109317368244\n",
    "- Test: 1.9522597174462015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18179109287485934, 1.952259624453726)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polynomial_regression(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is the optimal number of degrees for our polynomial features in this model? In general, how does increasing the polynomial degree relate to the Bias/Variance tradeoff?  (Note that this graph shows RMSE and not MSE.)\n",
    "\n",
    "<img src =\"visuals/rsme_poly_2.png\" width = \"600\">\n",
    "\n",
    "<!---\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "degree = list(range(1, 10 + 1))\n",
    "ax.plot(degree, error_train[0:len(degree)], \"-\", label=\"Train Error\")\n",
    "ax.plot(degree, error_test[0:len(degree)], \"-\", label=\"Test Error\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Polynomial Feature Degree\")\n",
    "ax.set_ylabel(\"Root Mean Squared Error\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Relationship Between Degree and Error\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"visuals/rsme_poly.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\")\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# From the above graph, it looks as though the optimal degree to select for polynomial feature engineering is 3. In particular,\n",
    "# we want to minimize overfitting on the training set, as this will produce higher error in the testing set. From the graph, beyond\n",
    "# degree 3, we see a sharp increase in the error on the test set while the error on the training set is going down. This indicates that\n",
    "# our model is fitting too well to the training data to be able to accurately predict on the testing data. Because the testing data\n",
    "# has not been touched by our model before initial predictions, ensuring accuracy on this set of data is critical to having a robust\n",
    "# model that can make predictions on new data. In general, bias is produced when the model isn't sensitive enough to features in the\n",
    "# training data to be able to make accurate predictions, and variance is produced when the model is too sensitive to these features,\n",
    "# so that a slight deviation from data in the model will produce a high error due to issues of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. In general what methods would you can to reduce overfitting and underfitting? Provide an example for both and explain how each technique works to reduce the problems of underfitting and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# To ensure that overfitting and underfitting are not issues, a major technique would be to employ the use of a validation set of data. \n",
    "# Essentially, this segmentation of data differs slightly from the testing set of data. The testing set is used to evaluate the accuracy \n",
    "# of the model on unknown data, but the results here certainly influence parameter selection, as we use the testing set of data as a \n",
    "# marker of model performance as we change parameters to find an optimal model. In contrast, a validation set is left completely untouched\n",
    "# until the final stage of evaluation. At this point, we have found an ``optimal'' model by training to train data, and by tuning \n",
    "# parameters until things also look good in the test data (not under or overfit). However, we cannot be sure until we test our final model\n",
    "# on validation data, which ensures that an issue of overfitting or underfitting to known values is handled. \n",
    "\n",
    "# This is a general strategy to improving model selection. In order to ensure that overfitting is not an issue, we might also look to\n",
    "# methods of regularization. In general, the idea behind these methods is to ``weight'' the different possible features so that the most\n",
    "# important features in terms of description of model variance are higher, and the ones where the description of model variance is lower\n",
    "# are weighted lower or ignored entirely. This has the effect of ensuring that small discrepancies that might be locally modeled by a\n",
    "# trend but don't actually contribute much to the model itself are not baked into the model, which would cause overfitting.\n",
    "\n",
    "# In order to ensure that underfitting is not an issue, we would need to perform some additional feature engineering, as an issue of\n",
    "# underfitting arises when the model fails to detect features that contribute to the variance in the data, or when these features are \n",
    "# not specifically coded into the model. In fact, we tried this kind of methodology above, where we produced polynomial features of the\n",
    "# given data. This adds complexity to the model, and more places for the model to assign changes in variance in the data. Of course,\n",
    "# there is a trade-off, as too many features will eventually contribute to overfitting, as demonstrated in the graph above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is the difference between the two types of regularization for linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# In linear regression there are two main types of regularization: L1 regularization and L2 regularization. These two methods are very\n",
    "# similar, and come down to a small mathematical change to the regularization term. In both cases, the regularization term is an additional\n",
    "# term that is added to the cost function to penalize features.\n",
    "\n",
    "# In L1 regularization, the regularization term is the sum of the absolute values of the model coefficients, multiplied by a constant \n",
    "# lambda. In L2 regularization, the regularization term is the sum of the squares of the model coefficients. These correspond to the L1\n",
    "# and L2 norms of the vectors composed of the model coefficients (mathemtically defined norms, essentially). In both cases, these\n",
    "# regularization terms serve to penalize features that don't contribute much to the variance in the data. The big difference comes in\n",
    "# the degree to which they are able to accomplish this. Mainly, L2 regularization will squash features which are not descriptive of the\n",
    "# data, but they will not fully remove them, as is accomplished when using L1 regularization. Sometimes this can be advantageous,\n",
    "# sometimes not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Why is scaling input variables a necessary step before regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# We want to scale input variables before regularization so that our lambda value has meaning in the context of our overall algorithm.\n",
    "# a scaling process, the value of the norm will be skewed by the magnitude of the features, and our rate constant lambda won't have\n",
    "# any appreciable effect on selection of a model or the gradient descent process. Also, scaling will make gradient descent faster overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series: Part 4 [Suggested Time: 10 min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "# load data\n",
    "ads_df = pd.read_csv(\"raw_data/social_network_ads.csv\")\n",
    "\n",
    "# one hot encode categorical feature\n",
    "def is_female(x):\n",
    "    \"\"\"Returns 1 if Female; else 0\"\"\"\n",
    "    if x == \"Female\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "ads_df[\"Female\"] = ads_df[\"Gender\"].apply(is_female)\n",
    "ads_df.drop([\"User ID\", \"Gender\"], axis=1, inplace=True)\n",
    "ads_df.head()\n",
    "\n",
    "# separate features and target\n",
    "X = ads_df.drop(\"Purchased\", axis=1)\n",
    "y = ads_df[\"Purchased\"]\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=19)\n",
    "\n",
    "# preprocessing\n",
    "scale = StandardScaler()\n",
    "scale.fit(X_train)\n",
    "X_train = scale.transform(X_train)\n",
    "X_test = scale.transform(X_test)\n",
    "\n",
    "# save preprocessed train/test split objects\n",
    "pickle.dump(X_train, open(\"write_data/social_network_ads/X_train_scaled.pkl\", \"wb\"))\n",
    "pickle.dump(X_test, open(\"write_data/social_network_ads/X_test_scaled.pkl\", \"wb\"))\n",
    "pickle.dump(y_train, open(\"write_data/social_network_ads/y_train.pkl\", \"wb\"))\n",
    "pickle.dump(y_test, open(\"write_data/social_network_ads/y_test.pkl\", \"wb\"))\n",
    "\n",
    "# build model\n",
    "model = LogisticRegression(C=1e5, solver=\"lbfgs\")\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# create confusion matrix\n",
    "# tn, fp, fn, tp\n",
    "cnf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "cnf_matrix\n",
    "\n",
    "# build confusion matrix plot\n",
    "plt.imshow(cnf_matrix,  cmap=plt.cm.Blues) #Create the basic matrix.\n",
    "\n",
    "# Add title and Axis Labels\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Add appropriate Axis Scales\n",
    "class_names = set(y_test) #Get class labels to add to matrix\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# Add Labels to Each Cell\n",
    "thresh = cnf_matrix.max() / 2. #Used for text coloring below\n",
    "#Here we iterate through the confusion matrix and append labels to our visualization.\n",
    "for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        plt.text(j, i, cnf_matrix[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "# Add a Side Bar Legend Showing Colors\n",
    "plt.colorbar()\n",
    "\n",
    "# Add padding\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"visuals/cnf_matrix.png\",\n",
    "            dpi=150,\n",
    "            bbox_inches=\"tight\")\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Which of the following can‚Äôt be a component for a time series plot?\n",
    "A) Seasonality <br>\n",
    "B) Trend <br>\n",
    "C) Cyclical <br>\n",
    "D) Noise<br>\n",
    "E)  None of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# I am assuming the answer is cyclical here, there can be parts of a time series component which are cyclical (such as seasonality, though\n",
    "# this is not guaranteed). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2) What does autocovariance measure?\n",
    "\n",
    "A) Linear dependence between multiple points on the different series observed at different times<br>\n",
    "B) Quadratic dependence between two points on the same series observed at different times<br>\n",
    "C) Linear dependence between two points on different series observed at same time<br>\n",
    "D) Linear dependence between two points on the same series observed at different times<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Looking at the below ACF plot, would you suggest to apply AR or MA in ARIMA modeling technique?\n",
    "\n",
    "![](visuals/acf.jpg)\n",
    "\n",
    "A) AR<br>\n",
    "B) MA <br>\n",
    "C) Can‚Äôt Say <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# A) AR (the series looks slightly underdifferenced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Stationarity is a desirable property for a time series process.\n",
    "\n",
    "A) TRUE <br>\n",
    "B) FALSE <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# A) True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Which of the following statement is correct?\n",
    "\n",
    "1. If autoregressive parameter (p) in an ARIMA model is 1, it means that there is no auto-correlation in the series.\n",
    "\n",
    "2. If moving average component (q) in an ARIMA model is 1, it means that there is auto-correlation in the series with lag 1.\n",
    "\n",
    "3. If integrated component (d) in an ARIMA model is 0, it means that the series is not stationary.\n",
    "\n",
    "A) Only 1 <br>\n",
    "B) Both 1 and 2 <br>\n",
    "C) Only 2 <br>\n",
    "D)  All of the statements <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# C) Only 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) BIC penalizes complex models more strongly than the AIC. \n",
    "\n",
    "A) TRUE <br>\n",
    "B) FALSE <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# A) TRUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) How many AR and MA terms should be included for the time series by looking at the above ACF and PACF plots?\n",
    "\n",
    "\n",
    "![](visuals/acf_pacf.png)\n",
    "\n",
    "\n",
    "A) AR (1) MA(0) <br>\n",
    "B) AR(0)MA(1) <br>\n",
    "C) AR(2)MA(1) <br>\n",
    "D) AR(1)MA(2) <br>\n",
    "E) Can‚Äôt Say <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "# ----\n",
    "# We want an MA term because lag 1 autocorrelation is negative through one lag\n",
    "# We want 0 AR terms because the PACF is negative through the first two lags\n",
    "# Hence we select B) AR(0)MA(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
