{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/msKNSs8rmJ5m/giphy.gif\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'images/thinking.jpeg'></img>\n",
    "\n",
    "#### Scenario: You wok for a international political consultant.  Your work is on the level and not shady at all.  Scott, a friendly coworker, is trying to sort through news articles to quickly filter political from non-political articles. He has heard you possess some solid NLP chops, and has asked for your help in automating the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div>\n",
    "<p float = 'left'> What type of problem is this?</p>\n",
    "<p float = 'left'> What steps do you anticipate carrying out? </p>\n",
    "<p float = 'left'> What challenges do you foresee? </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "<img src = 'https://media.giphy.com/media/WqLmcthJ7AgQKwYJbb/giphy.gif' alt=\"Drawing\" style=\"width: 300px;\"  float = 'right'> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vanilla Python Text Exploration\n",
    "\n",
    "Explore the texts in the example texts by:\n",
    "\n",
    "1. Creating a list of words in each text.\n",
    "2. Counting the number of occurences of the words.  \n",
    "3. Ordering the words by number of occurences.\n",
    "4. Comparing the counts\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "with open('text_examples/A.txt', 'r') as read_file:\n",
    "    text = read_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of words\n",
    "text = text.replace('\\n', '')\n",
    "tokens = text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'Reboot': 1,\n",
       "             'ordered': 2,\n",
       "             'for': 4,\n",
       "             'EU': 4,\n",
       "             'patent': 4,\n",
       "             'lawA': 1,\n",
       "             'European': 2,\n",
       "             'Parliament': 1,\n",
       "             'committee': 1,\n",
       "             'has': 3,\n",
       "             'a': 6,\n",
       "             'rewrite': 1,\n",
       "             'of': 6,\n",
       "             'the': 17,\n",
       "             'proposals': 2,\n",
       "             'controversial': 1,\n",
       "             'new': 2,\n",
       "             'Union': 1,\n",
       "             'rules': 1,\n",
       "             'which': 1,\n",
       "             'govern': 1,\n",
       "             'computer-based': 1,\n",
       "             'inventions.The': 1,\n",
       "             'Legal': 1,\n",
       "             'Affairs': 1,\n",
       "             'Committee': 1,\n",
       "             '(JURI)': 1,\n",
       "             'said': 2,\n",
       "             'Commission': 1,\n",
       "             'should': 1,\n",
       "             're-submit': 1,\n",
       "             'Computer': 1,\n",
       "             'Implemented': 1,\n",
       "             'Inventions': 1,\n",
       "             'Directive': 1,\n",
       "             'after': 1,\n",
       "             'MEPs': 2,\n",
       "             'failed': 1,\n",
       "             'to': 10,\n",
       "             'back': 1,\n",
       "             'it.': 1,\n",
       "             'It': 1,\n",
       "             'had': 2,\n",
       "             'vocal': 1,\n",
       "             'critics': 1,\n",
       "             'who': 1,\n",
       "             'say': 3,\n",
       "             'it': 3,\n",
       "             'could': 3,\n",
       "             'favour': 1,\n",
       "             'large': 1,\n",
       "             'over': 1,\n",
       "             'small': 2,\n",
       "             'firms': 2,\n",
       "             'and': 5,\n",
       "             'impact': 1,\n",
       "             'open-source': 1,\n",
       "             'software': 3,\n",
       "             'innovation.': 1,\n",
       "             'Supporters': 2,\n",
       "             'would': 3,\n",
       "             'let': 1,\n",
       "             'protect': 1,\n",
       "             'their': 2,\n",
       "             'inventions.': 1,\n",
       "             'The': 2,\n",
       "             'directive': 3,\n",
       "             'is': 2,\n",
       "             'intended': 1,\n",
       "             'offer': 1,\n",
       "             'protection': 1,\n",
       "             'inventions': 1,\n",
       "             'that': 3,\n",
       "             'use': 1,\n",
       "             'achieve': 1,\n",
       "             'effect,': 1,\n",
       "             'in': 7,\n",
       "             'other': 1,\n",
       "             'words,': 1,\n",
       "             '\"computer': 1,\n",
       "             'implemented': 1,\n",
       "             'invention\".': 1,\n",
       "             'draft': 3,\n",
       "             'law': 2,\n",
       "             'suffered': 1,\n",
       "             'setbacks': 1,\n",
       "             'when': 1,\n",
       "             'Poland,': 1,\n",
       "             'one': 2,\n",
       "             'largest': 1,\n",
       "             'member': 2,\n",
       "             'states,': 1,\n",
       "             'rejected': 1,\n",
       "             'its': 3,\n",
       "             'adoption': 1,\n",
       "             'twice': 1,\n",
       "             'two': 2,\n",
       "             'months.': 1,\n",
       "             'Intense': 1,\n",
       "             'lobbying': 1,\n",
       "             'on': 1,\n",
       "             'issue': 1,\n",
       "             'started': 1,\n",
       "             'gain': 1,\n",
       "             'momentum': 1,\n",
       "             'some': 1,\n",
       "             'national': 1,\n",
       "             'parliaments': 1,\n",
       "             'putting': 1,\n",
       "             'them': 1,\n",
       "             'under': 1,\n",
       "             'immense': 1,\n",
       "             'pressure.': 1,\n",
       "             'Only': 1,\n",
       "             'backed': 1,\n",
       "             'at': 1,\n",
       "             'JURI': 1,\n",
       "             'meeting,': 1,\n",
       "             'with': 2,\n",
       "             'voting': 1,\n",
       "             'abstain.Opponents': 1,\n",
       "             'welcomed': 1,\n",
       "             'decision': 1,\n",
       "             'first': 1,\n",
       "             'reading': 1,\n",
       "             'give': 1,\n",
       "             'chance': 1,\n",
       "             'have': 2,\n",
       "             'fuller': 1,\n",
       "             'debates': 1,\n",
       "             'about': 1,\n",
       "             'implications': 1,\n",
       "             'all': 1,\n",
       "             'states.': 1,\n",
       "             'In': 1,\n",
       "             'US,': 1,\n",
       "             'patenting': 1,\n",
       "             'computer': 1,\n",
       "             'programs': 1,\n",
       "             'internet': 1,\n",
       "             'business': 1,\n",
       "             'methods': 1,\n",
       "             'permitted.': 1,\n",
       "             'This': 1,\n",
       "             'means': 1,\n",
       "             'US-based': 1,\n",
       "             'Amazon.com': 1,\n",
       "             'holds': 1,\n",
       "             '\"one-click': 1,\n",
       "             'shopping\"': 1,\n",
       "             'service,': 1,\n",
       "             'example.': 1,\n",
       "             'Critics': 1,\n",
       "             'are': 2,\n",
       "             'concerned': 1,\n",
       "             'lead': 1,\n",
       "             'similar': 1,\n",
       "             'model': 1,\n",
       "             'happening': 1,\n",
       "             'Europe.': 1,\n",
       "             'This,': 1,\n",
       "             'they': 3,\n",
       "             'fear,': 1,\n",
       "             'hurt': 1,\n",
       "             'developers': 1,\n",
       "             'because': 1,\n",
       "             'do': 1,\n",
       "             'not': 1,\n",
       "             'legal': 2,\n",
       "             'financial': 1,\n",
       "             'might': 1,\n",
       "             'larger': 1,\n",
       "             'companies': 1,\n",
       "             'if': 1,\n",
       "             'fight': 1,\n",
       "             'action': 1,\n",
       "             'court.': 1,\n",
       "             'current': 1,\n",
       "             'laws': 2,\n",
       "             'inefficient': 1,\n",
       "             'serve': 1,\n",
       "             'even': 1,\n",
       "             'up': 1,\n",
       "             'playing': 1,\n",
       "             'field': 1,\n",
       "             'without': 1,\n",
       "             'bringing': 1,\n",
       "             'line': 1,\n",
       "             'US.': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of occurences\n",
    "token_counts = defaultdict(int)\n",
    "for t in tokens:\n",
    "    token_counts[t] += 1\n",
    "\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [f'{letter}.txt' for letter in string.ascii_uppercase[:12]]\n",
    "\n",
    "def vanilla_tokenizer(file_name):\n",
    "    token_counts = defaultdict(int)\n",
    "    with open(f'text_examples/{file_name}', 'r') as read_file:\n",
    "        text = read_file.read().replace('\\n', ' ')\n",
    "    \n",
    "    tokens = text.split(' ')\n",
    "    \n",
    "    for token in tokens:\n",
    "        token_counts[token] += 1\n",
    "        \n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>\"And</th>\n",
       "      <th>\"For</th>\n",
       "      <th>\"I</th>\n",
       "      <th>\"I'm</th>\n",
       "      <th>\"Jason</th>\n",
       "      <th>\"Recipients</th>\n",
       "      <th>\"Skype's</th>\n",
       "      <th>\"Take</th>\n",
       "      <th>\"This</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>year's</th>\n",
       "      <th>year,</th>\n",
       "      <th>year.</th>\n",
       "      <th>years</th>\n",
       "      <th>£1,000</th>\n",
       "      <th>£12m.</th>\n",
       "      <th>£3bn</th>\n",
       "      <th>£4m</th>\n",
       "      <th>£7,455.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A.txt</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B.txt</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C.txt</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D.txt</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E.txt</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F.txt</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>G.txt</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H.txt</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I.txt</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>J.txt</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K.txt</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L.txt</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 1261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          \"And  \"For   \"I  \"I'm  \"Jason  \"Recipients  \"Skype's  \"Take  \"This  \\\n",
       "A.txt  4   0.0   0.0  0.0   0.0     0.0          0.0       0.0    0.0    0.0   \n",
       "B.txt  4   0.0   0.0  0.0   0.0     0.0          0.0       0.0    0.0    0.0   \n",
       "C.txt  5   0.0   0.0  0.0   0.0     0.0          0.0       0.0    1.0    0.0   \n",
       "D.txt  4   0.0   0.0  0.0   0.0     0.0          0.0       0.0    0.0    0.0   \n",
       "E.txt  4   0.0   0.0  0.0   0.0     0.0          1.0       0.0    0.0    0.0   \n",
       "F.txt  3   0.0   0.0  0.0   0.0     0.0          0.0       0.0    0.0    0.0   \n",
       "G.txt  6   0.0   0.0  0.0   0.0     0.0          0.0       0.0    0.0    0.0   \n",
       "H.txt  3   0.0   0.0  0.0   0.0     0.0          0.0       0.0    0.0    0.0   \n",
       "I.txt  4   0.0   0.0  0.0   0.0     0.0          0.0       1.0    0.0    0.0   \n",
       "J.txt  3   1.0   0.0  1.0   0.0     0.0          0.0       0.0    0.0    0.0   \n",
       "K.txt  4   0.0   1.0  1.0   0.0     0.0          0.0       0.0    0.0    0.0   \n",
       "L.txt  3   0.0   0.0  1.0   2.0     1.0          0.0       0.0    0.0    1.0   \n",
       "\n",
       "       ...  year  year's  year,  year.  years  £1,000  £12m.  £3bn  £4m  \\\n",
       "A.txt  ...   0.0     0.0    0.0    0.0    0.0     0.0    0.0   0.0  0.0   \n",
       "B.txt  ...   0.0     0.0    1.0    0.0    0.0     0.0    0.0   1.0  0.0   \n",
       "C.txt  ...   1.0     2.0    0.0    1.0    0.0     1.0    0.0   0.0  0.0   \n",
       "D.txt  ...   0.0     0.0    0.0    0.0    0.0     0.0    0.0   0.0  0.0   \n",
       "E.txt  ...   0.0     0.0    0.0    0.0    0.0     0.0    0.0   0.0  0.0   \n",
       "F.txt  ...   0.0     0.0    0.0    0.0    1.0     0.0    0.0   0.0  0.0   \n",
       "G.txt  ...   1.0     0.0    0.0    0.0    0.0     0.0    0.0   0.0  0.0   \n",
       "H.txt  ...   1.0     0.0    0.0    0.0    0.0     0.0    1.0   0.0  1.0   \n",
       "I.txt  ...   0.0     0.0    0.0    1.0    0.0     0.0    0.0   0.0  0.0   \n",
       "J.txt  ...   0.0     0.0    0.0    1.0    0.0     0.0    0.0   0.0  0.0   \n",
       "K.txt  ...   0.0     0.0    0.0    0.0    1.0     0.0    0.0   0.0  0.0   \n",
       "L.txt  ...   2.0     0.0    0.0    0.0    0.0     0.0    0.0   0.0  0.0   \n",
       "\n",
       "       £7,455.  \n",
       "A.txt      0.0  \n",
       "B.txt      0.0  \n",
       "C.txt      1.0  \n",
       "D.txt      0.0  \n",
       "E.txt      0.0  \n",
       "F.txt      0.0  \n",
       "G.txt      0.0  \n",
       "H.txt      0.0  \n",
       "I.txt      0.0  \n",
       "J.txt      0.0  \n",
       "K.txt      0.0  \n",
       "L.txt      0.0  \n",
       "\n",
       "[12 rows x 1261 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df = pd.DataFrame()\n",
    "for f in file_names:\n",
    "    f_token_df = pd.DataFrame(vanilla_tokenizer(f), index=[f'{f}'])\n",
    "    word_df = pd.concat([word_df, f_token_df], axis=0, sort=True)\n",
    "\n",
    "word_df.fillna(value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Bag of Words</h2>\n",
    "\n",
    "<img src = \"images/bag_of_word.jpg\"></img>\n",
    "\n",
    "What is the problem with text in relation to machine learning?\n",
    "BOW takes a text, breaks it up into small pieces (words, bigrams, stems, lemma), an converts it into counts.  These counts can then be fed into our familiar machine learning algorithms.\n",
    "\n",
    "Question: Did any algorithm pop into your head that might be particularly suited to bags of words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Steps for creating a bag of words\n",
    "\n",
    "1. make lowercase \n",
    "2. remove punctuation\n",
    "3. remove stopwords\n",
    "4. apply stemmer/lemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To help us with the above steps, we will introduce a new library, **NLTK**  [documentation](https://www.nltk.org/).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#We will come back to the articles later, but to practice preparing a text, \n",
    "#let's use another of the NLTK resources (https://www.nltk.org/book/ch02.html)\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, regexp_tokenize\n",
    "nltk.download('gutenberg')\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "text = nltk.corpus.gutenberg.raw('carroll-alice.txt').replace('\\n',' ')[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/flatironschool/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "## tokenize the text\n",
    "nltk.download('punkt')\n",
    "text_tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# or use regexp which can take care of punctuation removal as well.\n",
    "#https://regexr.com/\n",
    "pattern = (\"([a-zA-Z]+(?:'[a-z]+)?)\")\n",
    "tokenize = regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Alice's\",\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'by',\n",
       " 'Lewis',\n",
       " 'Carroll',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'Down',\n",
       " 'the',\n",
       " 'Rabbit',\n",
       " 'Hole',\n",
       " 'Alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get',\n",
       " 'very',\n",
       " 'tired',\n",
       " 'of',\n",
       " 'sitting',\n",
       " 'by',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'on',\n",
       " 'the',\n",
       " 'bank',\n",
       " 'and',\n",
       " 'of',\n",
       " 'having',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'do',\n",
       " 'once',\n",
       " 'or',\n",
       " 'twice',\n",
       " 'she',\n",
       " 'had',\n",
       " 'peeped',\n",
       " 'into',\n",
       " 'the',\n",
       " 'book',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'was',\n",
       " 'reading',\n",
       " 'but',\n",
       " 'it',\n",
       " 'had',\n",
       " 'no',\n",
       " 'pictures',\n",
       " 'or',\n",
       " 'conversations',\n",
       " 'in',\n",
       " 'it',\n",
       " 'and',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'a',\n",
       " 'book',\n",
       " 'thought',\n",
       " 'Alice',\n",
       " 'without',\n",
       " 'pictures',\n",
       " 'or',\n",
       " 'conversation',\n",
       " 'So',\n",
       " 'she',\n",
       " 'was',\n",
       " 'considering',\n",
       " 'in',\n",
       " 'her',\n",
       " 'own',\n",
       " 'mind',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'she',\n",
       " 'could',\n",
       " 'for',\n",
       " 'the',\n",
       " 'hot',\n",
       " 'day',\n",
       " 'made',\n",
       " 'her',\n",
       " 'feel',\n",
       " 'very',\n",
       " 'sleepy',\n",
       " 'and',\n",
       " 'stupid',\n",
       " 'whether',\n",
       " 'the',\n",
       " 'pleasure',\n",
       " 'of',\n",
       " 'making',\n",
       " 'a',\n",
       " 'daisy',\n",
       " 'chain',\n",
       " 'would',\n",
       " 'be',\n",
       " 'worth',\n",
       " 'the',\n",
       " 'trouble',\n",
       " 'of',\n",
       " 'getting',\n",
       " 'up',\n",
       " 'and',\n",
       " 'picking',\n",
       " 'the',\n",
       " 'daisies',\n",
       " 'when',\n",
       " 'suddenly',\n",
       " 'a',\n",
       " 'White',\n",
       " 'Rabbit',\n",
       " 'with',\n",
       " 'pink',\n",
       " 'eyes',\n",
       " 'ran',\n",
       " 'close',\n",
       " 'by',\n",
       " 'her',\n",
       " 'There',\n",
       " 'was',\n",
       " 'nothing',\n",
       " 'so',\n",
       " 'VERY',\n",
       " 'remarkable',\n",
       " 'in',\n",
       " 'that',\n",
       " 'nor',\n",
       " 'did',\n",
       " 'Alice',\n",
       " 'think',\n",
       " 'it',\n",
       " 'so',\n",
       " 'VERY',\n",
       " 'much',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'way',\n",
       " 'to',\n",
       " 'hear',\n",
       " 'the',\n",
       " 'Rabbit',\n",
       " 'say',\n",
       " 'to',\n",
       " 'itself',\n",
       " 'Oh',\n",
       " 'dear',\n",
       " 'Oh',\n",
       " 'dear',\n",
       " 'I',\n",
       " 'shall',\n",
       " 'be',\n",
       " 'late',\n",
       " 'when',\n",
       " 'she',\n",
       " 'thought',\n",
       " 'it',\n",
       " 'over',\n",
       " 'afterwards',\n",
       " 'it',\n",
       " 'occurred',\n",
       " 'to',\n",
       " 'her',\n",
       " 'that',\n",
       " 'she',\n",
       " 'ought',\n",
       " 'to',\n",
       " 'have',\n",
       " 'wondered',\n",
       " 'at',\n",
       " 'this',\n",
       " 'but',\n",
       " 'at',\n",
       " 'the',\n",
       " 'time',\n",
       " 'it',\n",
       " 'all',\n",
       " 'seemed',\n",
       " 'quite',\n",
       " 'natural',\n",
       " 'but',\n",
       " 'when',\n",
       " 'the',\n",
       " 'Rabbit',\n",
       " 'actually',\n",
       " 'TOOK',\n",
       " 'A',\n",
       " 'WATCH',\n",
       " 'OUT',\n",
       " 'OF',\n",
       " 'ITS',\n",
       " 'WAISTCOAT',\n",
       " 'POCKET',\n",
       " 'and',\n",
       " 'looked',\n",
       " 'at',\n",
       " 'it',\n",
       " 'and',\n",
       " 'then',\n",
       " 'hurried',\n",
       " 'on',\n",
       " 'Alice',\n",
       " 'started',\n",
       " 'to',\n",
       " 'her',\n",
       " 'feet',\n",
       " 'for',\n",
       " 'it',\n",
       " 'flashed',\n",
       " 'across',\n",
       " 'her',\n",
       " 'mind',\n",
       " 'that',\n",
       " 'she',\n",
       " 'had',\n",
       " 'never',\n",
       " 'before',\n",
       " 'seen',\n",
       " 'a',\n",
       " 'rabbit',\n",
       " 'with',\n",
       " 'either',\n",
       " 'a',\n",
       " 'waistcoat',\n",
       " 'pocket',\n",
       " 'or',\n",
       " 'a',\n",
       " 'watch',\n",
       " 'to',\n",
       " 'take',\n",
       " 'out',\n",
       " 'of',\n",
       " 'it',\n",
       " 'and',\n",
       " 'burning',\n",
       " 'with',\n",
       " 'curiosity',\n",
       " 'she',\n",
       " 'ran',\n",
       " 'across',\n",
       " 'the',\n",
       " 'field',\n",
       " 'after',\n",
       " 'it',\n",
       " 'and',\n",
       " 'fortunately',\n",
       " 'was',\n",
       " 'just',\n",
       " 'in',\n",
       " 'time',\n",
       " 'to',\n",
       " 'see',\n",
       " 'it',\n",
       " 'pop',\n",
       " 'down',\n",
       " 'a',\n",
       " 'large',\n",
       " 'rabbit',\n",
       " 'hole',\n",
       " 'under',\n",
       " 'the',\n",
       " 'hedge',\n",
       " 'In',\n",
       " 'another',\n",
       " 'moment',\n",
       " 'down',\n",
       " 'went',\n",
       " 'Alice',\n",
       " 'after',\n",
       " 'it',\n",
       " 'never',\n",
       " 'once',\n",
       " 'considering',\n",
       " 'how',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " 'she',\n",
       " 'was',\n",
       " 'to',\n",
       " 'get',\n",
       " 'out',\n",
       " 'again',\n",
       " 'The',\n",
       " 'rabbit',\n",
       " 'hole',\n",
       " 'went',\n",
       " 'straight',\n",
       " 'on',\n",
       " 'like',\n",
       " 'a',\n",
       " 'tunnel',\n",
       " 'for',\n",
       " 'some',\n",
       " 'way',\n",
       " 'and',\n",
       " 'then',\n",
       " 'dipped',\n",
       " 'suddenly',\n",
       " 'down',\n",
       " 'so',\n",
       " 'suddenly',\n",
       " 'that',\n",
       " 'Alice',\n",
       " 'had',\n",
       " 'not',\n",
       " 'a',\n",
       " 'moment',\n",
       " 'to',\n",
       " 'think',\n",
       " 'about',\n",
       " 'stopping',\n",
       " 'herself',\n",
       " 'before',\n",
       " 'she',\n",
       " 'found',\n",
       " 'herself',\n",
       " 'falling',\n",
       " 'down',\n",
       " 'a',\n",
       " 'very',\n",
       " 'deep',\n",
       " 'well',\n",
       " 'Either',\n",
       " 'the',\n",
       " 'well',\n",
       " 'was',\n",
       " 'very',\n",
       " 'deep',\n",
       " 'or',\n",
       " 'she',\n",
       " 'fell',\n",
       " 'very',\n",
       " 'slowly',\n",
       " 'for',\n",
       " 'she',\n",
       " 'had',\n",
       " 'plenty',\n",
       " 'of',\n",
       " 'time',\n",
       " 'as',\n",
       " 'she',\n",
       " 'went',\n",
       " 'down',\n",
       " 'to',\n",
       " 'look',\n",
       " 'about',\n",
       " 'her',\n",
       " 'and',\n",
       " 'to',\n",
       " 'wonder',\n",
       " 'what',\n",
       " 'was',\n",
       " 'going',\n",
       " 'to',\n",
       " 'happen',\n",
       " 'next',\n",
       " 'First',\n",
       " 'she',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'look',\n",
       " 'down',\n",
       " 'and',\n",
       " 'make',\n",
       " 'out',\n",
       " 'what',\n",
       " 'she',\n",
       " 'was',\n",
       " 'coming',\n",
       " 'to',\n",
       " 'but',\n",
       " 'it',\n",
       " 'was',\n",
       " 'too',\n",
       " 'dar']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Make Lowercase\n",
    "token_lowercase = [t.lower() for t in tokenize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/flatironschool/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 2. Remove punctuation\n",
    "# 3. Remove Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "# stopwords.words()\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lowercase = [t for t in token_lowercase if t not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stemmers/Lemmatizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caress',\n",
       " 'fli',\n",
       " 'die',\n",
       " 'mule',\n",
       " 'deni',\n",
       " 'die',\n",
       " 'agre',\n",
       " 'own',\n",
       " 'humbl',\n",
       " 'size',\n",
       " 'meet',\n",
       " 'state',\n",
       " 'siez',\n",
       " 'item',\n",
       " 'sensat',\n",
       " 'tradit',\n",
       " 'refer',\n",
       " 'colon',\n",
       " 'plot']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Stemmers/Lemmatizers\n",
    "## why would I use a stemmer and not a lemmatizer at all times?\n",
    "# raw_text = nltk.corpus.gutenberg.raw()\n",
    "from nltk.stem import *\n",
    "\n",
    "\n",
    "example = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "          'plotted']\n",
    "\n",
    "p_stem = PorterStemmer()\n",
    "stemmed_words = [p_stem.stem(word) for word in example]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Porter Stemmer: Least aggressive stemmer.\n",
    "Snowball stemmer: more aggressive in how it stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/flatironschool/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agreed'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatizers\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "example = ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "           'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "           'meeting', 'stating', 'siezing', 'itemization',\n",
    "           'sensational', 'traditional', 'reference', 'colonizer',\n",
    "          'plotted']\n",
    "\n",
    "from nltk import pos_tag\n",
    "# Problem with POS\n",
    "wordnet_lemmatizer.lemmatize(example[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Frequency Distributions the easy way\n",
    "\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing using data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = ['the bread week winner goes to the final', \n",
    "        'the winner never has a soggy bottom', \n",
    "        'the contestants at the bottom were poor bread bakers' ]\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(docs)\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Each word is a dimension!\n",
    "\n",
    "![word](https://media.giphy.com/media/xT1R9ERHwyzbCkIwla/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
